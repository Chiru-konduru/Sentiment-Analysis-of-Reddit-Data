{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nnltk.download('punkt')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-31T20:52:59.622260Z","iopub.execute_input":"2022-08-31T20:52:59.623022Z","iopub.status.idle":"2022-08-31T20:53:02.064936Z","shell.execute_reply.started":"2022-08-31T20:52:59.622903Z","shell.execute_reply":"2022-08-31T20:53:02.064051Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/reddit-wallstreetsbets-posts/reddit_wsb.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:02.066930Z","iopub.execute_input":"2022-08-31T20:53:02.067343Z","iopub.status.idle":"2022-08-31T20:53:03.300429Z","shell.execute_reply.started":"2022-08-31T20:53:02.067308Z","shell.execute_reply":"2022-08-31T20:53:03.299363Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"title_data = data[['title','timestamp']].copy()\n\ntitle_data = title_data.dropna()\n\n#Lower-case all post\ntitle_data.title = title_data.title.str.lower()\n\n#Remove handlers\ntitle_data.title = title_data.title.apply(lambda x:re.sub('@[^\\s]+','',x))\n\n# Remove URLS\ntitle_data.title = title_data.title.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n\n# Remove all the special characters\ntitle_data.title = title_data.title.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n\n#remove all single characters\ntitle_data.title = title_data.title.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n\n# Substituting multiple spaces with single space\ntitle_data.title = title_data.title.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n\n#Remove Time From Timestamp\ntitle_data.timestamp = pd.to_datetime(title_data.timestamp).dt.date","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:03.302021Z","iopub.execute_input":"2022-08-31T20:53:03.302529Z","iopub.status.idle":"2022-08-31T20:53:04.254281Z","shell.execute_reply.started":"2022-08-31T20:53:03.302485Z","shell.execute_reply":"2022-08-31T20:53:04.253485Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sid = SIA()\n\ntitle_data['sentiments']           = title_data['title'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\ntitle_data['Positive Sentiment']   = title_data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \ntitle_data['Neutral Sentiment']    = title_data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\ntitle_data['Negative Sentiment']   = title_data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\ntitle_data.drop(columns=['sentiments'],inplace=True)\ntitle_data.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:04.255542Z","iopub.execute_input":"2022-08-31T20:53:04.255844Z","iopub.status.idle":"2022-08-31T20:53:13.617661Z","shell.execute_reply.started":"2022-08-31T20:53:04.255817Z","shell.execute_reply":"2022-08-31T20:53:13.616802Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# creating class from sentiment values\ntitle_data['class_name'] = title_data[['Positive Sentiment', 'Neutral Sentiment', 'Negative Sentiment']].idxmax(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:13.620978Z","iopub.execute_input":"2022-08-31T20:53:13.621267Z","iopub.status.idle":"2022-08-31T20:53:13.698735Z","shell.execute_reply.started":"2022-08-31T20:53:13.621240Z","shell.execute_reply":"2022-08-31T20:53:13.697846Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# removing neutral class from reducing bias\ndata = title_data[title_data['class_name'] != 'Neutral Sentiment']\ndata.reset_index(drop=True, inplace=True)\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:13.701265Z","iopub.execute_input":"2022-08-31T20:53:13.701582Z","iopub.status.idle":"2022-08-31T20:53:13.726896Z","shell.execute_reply.started":"2022-08-31T20:53:13.701553Z","shell.execute_reply":"2022-08-31T20:53:13.726173Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# total number of positive and negative sentiments\nl = len(data[data['class_name'] == 'Negative Sentiment'])\nprint(f\"There are {l} negative sentences\")\n\nm = len(data[data['class_name'] == 'Positive Sentiment'])\nprint(f\"There are {m} positive sentences\")","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:13.730098Z","iopub.execute_input":"2022-08-31T20:53:13.730515Z","iopub.status.idle":"2022-08-31T20:53:13.739504Z","shell.execute_reply.started":"2022-08-31T20:53:13.730481Z","shell.execute_reply":"2022-08-31T20:53:13.738256Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# replacing negative and positive sentiment in class with '0' and '1'\ndata['class_name'][data['class_name'] == 'Negative Sentiment'] = 0\ndata['class_name'][data['class_name'] == 'Positive Sentiment'] = 1\n\ndata.rename(columns={'class_name':'class'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:13.740855Z","iopub.execute_input":"2022-08-31T20:53:13.741231Z","iopub.status.idle":"2022-08-31T20:53:13.756542Z","shell.execute_reply.started":"2022-08-31T20:53:13.741192Z","shell.execute_reply":"2022-08-31T20:53:13.755621Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Plotting Positive and Negative Sentiments\nX = data[['Positive Sentiment', 'Negative Sentiment']].values\nY = data['class'].values;\n\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red' , 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:,0], X[:,1], c=[colors[int(k)] for k in Y], s=0.1)\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Graph shows that the positive and negative sentiments are majorly separated so theaccuracy should be high","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:13.758206Z","iopub.execute_input":"2022-08-31T20:53:13.758950Z","iopub.status.idle":"2022-08-31T20:53:14.001049Z","shell.execute_reply.started":"2022-08-31T20:53:13.758894Z","shell.execute_reply":"2022-08-31T20:53:14.000044Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data = data[['title','class']].copy()\ndata.head()\n\n# data ready ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:14.002193Z","iopub.execute_input":"2022-08-31T20:53:14.002506Z","iopub.status.idle":"2022-08-31T20:53:14.015389Z","shell.execute_reply.started":"2022-08-31T20:53:14.002476Z","shell.execute_reply":"2022-08-31T20:53:14.014365Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# creating training and testing data\nsplit_ratio = int(len(data) * 0.8)\ntrain_x = data['title'][:split_ratio]\ntest_x = data['title'][split_ratio:]\ntrain_y = data['class'][:split_ratio]\ntest_y = data['class'][split_ratio:]","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:14.017339Z","iopub.execute_input":"2022-08-31T20:53:14.017660Z","iopub.status.idle":"2022-08-31T20:53:14.023765Z","shell.execute_reply.started":"2022-08-31T20:53:14.017629Z","shell.execute_reply":"2022-08-31T20:53:14.022539Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_y = np.asarray(train_y, dtype='int32')\ntest_y = np.asarray(test_y, dtype='int32')\n\ntrain_y = train_y.reshape(-1, 1)\ntest_y = test_y.reshape(-1,1)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:14.025139Z","iopub.execute_input":"2022-08-31T20:53:14.025560Z","iopub.status.idle":"2022-08-31T20:53:14.033343Z","shell.execute_reply.started":"2022-08-31T20:53:14.025518Z","shell.execute_reply":"2022-08-31T20:53:14.032256Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# removing stopwords and stemming for better accuracy\ndef preprocess(sentence):\n    \n    sentence = sentence.lower()\n    stemmer = PorterStemmer()\n    \n    tokenized = word_tokenize(sentence)\n    cleaned_list = []\n    \n    stopwords_english = stopwords.words('english')\n    \n    for word in tokenized:\n        if (word not in stopwords_english and word not in string.punctuation):\n            stem_word = stemmer.stem(word)  \n            cleaned_list.append(stem_word)\n            \n    return cleaned_list","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:14.034328Z","iopub.execute_input":"2022-08-31T20:53:14.034599Z","iopub.status.idle":"2022-08-31T20:53:14.044443Z","shell.execute_reply.started":"2022-08-31T20:53:14.034572Z","shell.execute_reply":"2022-08-31T20:53:14.043399Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def build_freqs(sentences, ys):\n\n    yslist = np.squeeze(ys).tolist()\n\n    freqs = {}\n    for y, sentence in zip(yslist, sentences):\n        for word in preprocess(sentence):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs\n\nfreqs = build_freqs(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:14.045685Z","iopub.execute_input":"2022-08-31T20:53:14.046102Z","iopub.status.idle":"2022-08-31T20:53:15.285613Z","shell.execute_reply.started":"2022-08-31T20:53:14.046060Z","shell.execute_reply":"2022-08-31T20:53:15.284858Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def sigmoid(z): \n    return (1 / (1 + np.exp(-z)))","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:15.286981Z","iopub.execute_input":"2022-08-31T20:53:15.287395Z","iopub.status.idle":"2022-08-31T20:53:15.292373Z","shell.execute_reply.started":"2022-08-31T20:53:15.287349Z","shell.execute_reply":"2022-08-31T20:53:15.291393Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def gradientDescent(x, y, theta, alpha, num_iters):\n\n    m = len(x)\n    \n    for i in range(0, num_iters):\n        z = np.dot(x, theta)\n        h = sigmoid(z)\n        \n        J = (-1/m) * (np.dot((y.T), np.log(h)) + np.dot((1 - y).T, np.log(1 - h))) \n        \n        theta = theta - (alpha/m) * np.dot(x.T, (h - y))\n        \n        if i % (num_iters) == 0:\n            print(i, J)\n    \n    J = float(J)\n    \n    return J, theta","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:15.294156Z","iopub.execute_input":"2022-08-31T20:53:15.294608Z","iopub.status.idle":"2022-08-31T20:53:15.304170Z","shell.execute_reply.started":"2022-08-31T20:53:15.294567Z","shell.execute_reply":"2022-08-31T20:53:15.303378Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def extract_features(sentence, freqs):\n\n    word_l = preprocess(sentence)\n    x = np.zeros((1, 3)) \n    \n    x[0,0] = 1 \n    \n    for word in word_l:            \n        x[0,1] += freqs[(word, 1)] if (word, 1) in freqs else 0\n        \n        x[0,2] += freqs[(word, 0)] if (word, 0.0) in freqs else 0 \n\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:15.305133Z","iopub.execute_input":"2022-08-31T20:53:15.305467Z","iopub.status.idle":"2022-08-31T20:53:15.317088Z","shell.execute_reply.started":"2022-08-31T20:53:15.305436Z","shell.execute_reply":"2022-08-31T20:53:15.316355Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"X = np.zeros((len(train_x), 3))\nfor i in range(len(train_x)):\n    X[i, :]= extract_features(train_x[i], freqs)\n\nY = train_y\n\nJ, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-5, 15000)\n\nprint(f\"The cost after training is {J:.8f}.\")\n\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:15.318083Z","iopub.execute_input":"2022-08-31T20:53:15.318515Z","iopub.status.idle":"2022-08-31T20:53:21.358419Z","shell.execute_reply.started":"2022-08-31T20:53:15.318482Z","shell.execute_reply":"2022-08-31T20:53:21.357531Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def predict(sentence, freqs, theta):\n\n    x = extract_features(sentence, freqs)\n\n    y_pred = sigmoid(np.dot(x, theta))\n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:21.359835Z","iopub.execute_input":"2022-08-31T20:53:21.360227Z","iopub.status.idle":"2022-08-31T20:53:21.365429Z","shell.execute_reply.started":"2022-08-31T20:53:21.360183Z","shell.execute_reply":"2022-08-31T20:53:21.364411Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"for text in ['I am happy', 'very bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n    print( '%s -> %f' % (text, predict(text, freqs, theta)))","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:21.366887Z","iopub.execute_input":"2022-08-31T20:53:21.367516Z","iopub.status.idle":"2022-08-31T20:53:21.387086Z","shell.execute_reply.started":"2022-08-31T20:53:21.367474Z","shell.execute_reply":"2022-08-31T20:53:21.386031Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def test_logistic_regression(test_x, test_y, freqs, theta):\n\n    y_hat = []\n    \n    for sentence in test_x:\n        y_pred = predict(sentence, freqs, theta)\n        \n        if y_pred > 0.5:\n            y_hat.append(1.0)\n        else:\n            y_hat.append(0.0)\n\n    accuracy = (np.array((y_hat)) == np.squeeze(test_y)).mean()\n\n    return accuracy\n\ntmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\nprint(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:21.388790Z","iopub.execute_input":"2022-08-31T20:53:21.389242Z","iopub.status.idle":"2022-08-31T20:53:21.762882Z","shell.execute_reply.started":"2022-08-31T20:53:21.389198Z","shell.execute_reply":"2022-08-31T20:53:21.761899Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"text = 'Gamestop is high'\nprint(preprocess(text))\ny_hat = predict(text, freqs, theta)\nprint(y_hat)\nif y_hat > 0.5:\n    print('Positive sentiment')\nelse: \n    print('Negative sentiment')","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:53:21.764616Z","iopub.execute_input":"2022-08-31T20:53:21.765027Z","iopub.status.idle":"2022-08-31T20:53:21.774089Z","shell.execute_reply.started":"2022-08-31T20:53:21.764984Z","shell.execute_reply":"2022-08-31T20:53:21.772746Z"},"trusted":true},"execution_count":22,"outputs":[]}]}